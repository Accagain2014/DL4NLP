
2020. Natural Language Processing with Deep Learning

### Links
- [2020 spring xixiaoyao/CS224n-winter-together](https://github.com/xixiaoyao/CS224n-winter-together)
- [website](http://web.stanford.edu/class/cs224n/)
- [videos](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)



### Table of Contents
- lecture 1: Introduction and Word Vectors
- lecture 2: Word Vectors 2 and Word Senses
- lecture 3: Word Window Classification, Neural Networks, and PyTorch
- lecture 4: Matrix Calculus and Backpropagation
- lecture 5: Linguistic Structure: Dependency Parsing



### homeworks
- HW1 is hopefully an easy on ramp – an IPython Notebook
- HW2 is pure Python (numpy) but expects you to do (multivariate) calculus so you really understand the basics
- HW3 introduces PyTorch
- HW4 and HW5 use PyTorch on a GPU (Microsoft Azure)
    - Libraries like PyTorch, Tensorflow (and Chainer, MXNet, CNTK, Keras, etc.) are becoming the standard tools of DL
- For FP, you either
    - Do the default project, which is SQuAD question answering
    - Open-ended but an easier start; a good choice for most
    - Propose a custom final project, which we approve You will receive feedback from a mentor (TA/prof/postdoc/PhD)
    - Can work in teams of 1–3; can use any language
    

